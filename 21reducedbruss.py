import numpy as np
import matplotlib.pyplot as plt

# ----------------------------------------------------------------------------
# 1) Problem definitions
# ----------------------------------------------------------------------------
def brusselator(t, Y, A=1.0, B=3.0):
    u, v = Y
    du = A - (B + 1)*u + u*u*v
    dv =   B*u     - u*u*v
    return np.array([du, dv])

def linear_vec(t, Y):
    # u' = u, Y is array of length 1, solution Y = exp(t)
    return Y

# ----------------------------------------------------------------------------
# 2) ESRK-21 stepper and solver
# ----------------------------------------------------------------------------
def esrk21_step(f, t, y, h, a, b):
    s = len(b)
    k = [None]*s
    for i in range(s):
        c = np.sum(a[i,:i])
        yi = y.copy()
        for j in range(i):
            yi += h * a[i,j] * k[j]
        k[i] = f(t + c*h, yi)
    yn = y.copy()
    for i in range(s):
        yn += h * b[i] * k[i]
    return yn

def esrk21_solve(f, t0, y0, t1, h, a, b):
    y = y0.copy()
    N = int(np.ceil((t1-t0)/h))
    h_actual = (t1-t0)/N
    t = t0
    for _ in range(N):
        y = esrk21_step(f, t, y, h_actual, a, b)
        t += h_actual
    return y, N

# ----------------------------------------------------------------------------
# 3) Analysis: convergence, regression, work-precision
# ----------------------------------------------------------------------------
def analyze_convergence(f, y0, t0, t1, a, b):
    s = len(b)
    h_vals = np.array([2**(-k) for k in range(4, 11)])
    errors = []
    costs = []

    # Convergence with Richardson extrapolation
    for h in h_vals:
        y_h,  N_h  = esrk21_solve(f, t0, y0, t1, h,   a, b)
        y_h2, N_h2 = esrk21_solve(f, t0, y0, t1, h/2, a, b)
        # Richardson: order=3 => factor=8
        y_ref = (8*y_h2 - y_h)/(8 - 1)
        err = np.linalg.norm(y_h2 - y_ref, ord=np.inf)
        errors.append(err)
        costs.append(s * N_h2)  # cost = f calls at finer h

    errors = np.array(errors)
    costs = np.array(costs)

    # Regression on log-log
    x = np.log(h_vals)
    y_log = np.log(errors)
    m, c = np.polyfit(x, y_log, 1)
    y_pred = m*x + c
    resid = y_log - y_pred
    ss_res = np.sum(resid**2)
    ss_tot = np.sum((y_log - np.mean(y_log))**2)
    R2 = 1 - ss_res/ss_tot
    n = len(h_vals)
    se_m = np.sqrt(ss_res/(n-2)/np.sum((x - np.mean(x))**2))

    # Print results
    print(f"{'h':>10s} | {'Error':>12s} | {'p_obs':>6s} | {'Cost':>6s}")
    print("-"*44)
    for i, h in enumerate(h_vals):
        if i < len(h_vals)-1:
            p_obs = np.log(errors[i]/errors[i+1]) / np.log(h_vals[i]/h_vals[i+1])
            p_str = f"{p_obs:.3f}"
        else:
            p_str = "   -  "
        print(f"{h:10.5f} | {errors[i]:12.3e} | {p_str:>6s} | {costs[i]:6d}")
    print(f"\nRegression slope = {m:.4f} ± {se_m:.4f}, R² = {R2:.5f}\n")

    # Convergence plot
    plt.figure(figsize=(5,4))
    plt.loglog(h_vals, errors, 'o-', label="Error")
    plt.loglog(h_vals, np.exp(c)*h_vals**m, '--', label=f"Slope={m:.2f}")
    plt.xlabel("h"); plt.ylabel("Error")
    plt.title("Convergence")
    plt.grid(True, which="both", ls=":")
    plt.legend()
    plt.show()

    # Work-precision diagram
    plt.figure(figsize=(5,4))
    plt.loglog(costs, errors, 's-')
    plt.xlabel("Function calls"); plt.ylabel("Error")
    plt.title("Work-Precision")
    plt.grid(True, which="both", ls=":")
    plt.show()

# ----------------------------------------------------------------------------
# 4) Coefficients (fill in your full 21×21 'a' matrix and length-21 'b' vector)
# ----------------------------------------------------------------------------
a = np.array([
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0477859117523170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, -0.000342225369733892, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, -0.0379306642681654, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0713548421395141, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259359352931570, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, -0.00953495091906422, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0904519523018936, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, -0.000396135089732896, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, -0.153935717033075, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0106794630936747, 0.000795951292330683, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0106794630936747, -0.115335444191199, -0.119588952205909, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0106794630936747, -0.115335444191199, 0.157354569317741, 0.164687679052309, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0106794630936747, -0.115335444191199, 0.157354569317741, 0.0996953916040489, -0.151151371693320, 0, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0106794630936747, -0.115335444191199, 0.157354569317741, 0.0996953916040489, 0.124626570680465, -0.185777493787929, 0, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0106794630936747, -0.115335444191199, 0.157354569317741, 0.0996953916040489, 0.124626570680465, 0.0999254297810373, 0.181570806943121, 0, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0106794630936747, -0.115335444191199, 0.157354569317741, 0.0996953916040489, 0.124626570680465, 0.0999254297810373, -0.157721301562393, 9.54651547687642e-5, 0, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0106794630936747, -0.115335444191199, 0.157354569317741, 0.0996953916040489, 0.124626570680465, 0.0999254297810373, -0.157721301562393, 0.171838581104214, 0.188961619753159, 0, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0106794630936747, -0.115335444191199, 0.157354569317741, 0.0996953916040489, 0.124626570680465, 0.0999254297810373, -0.157721301562393, 0.171838581104214, -0.159282253882384, 0.163589906237245, 0, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0106794630936747, -0.115335444191199, 0.157354569317741, 0.0996953916040489, 0.124626570680465, 0.0999254297810373, -0.157721301562393, 0.171838581104214, -0.159282253882384, 0.153692305711512, -0.000244631681385317, 0, 0],
    [0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0259186933858971, 0.0106794630936747, -0.115335444191199, 0.157354569317741, 0.0996953916040489, 0.124626570680465, 0.0999254297810373, -0.157721301562393, 0.171838581104214, -0.159282253882384, 0.153692305711512, 0.135802482016176, -0.146394354124576, 0]
], dtype=np.float128)

b = np.array([
    0.025918693385897126,  0.025918693385897126,  0.025918693385897126,
    0.025918693385897126,  0.025918693385897126,  0.025918693385897126,
    0.025918693385897126,  0.025918693385897126,  0.010679463093674657,
   -0.11533544419119937,   0.15735456931774092,   0.09969539160404886,
    0.12462657068046491,   0.09992542978103731,  -0.1577213015623934,
    0.17183858110421352,  -0.1592822538823839,    0.15369230571151177,
    0.13580248201617606,   0.12674481627127573,   0.14462984296865586
], dtype=np.float128)
# ----------------------------------------------------------------------------
# 5) Run analyses
# ----------------------------------------------------------------------------
if __name__ == "__main__":
    print("=== Brusselator Test ===")
    analyze_convergence(brusselator, np.array([1.0,1.0]), 0.0, 1.0, a, b)
    print("\n=== Linear Test u'=u ===")
    analyze_convergence(linear_vec, np.array([1.0]), 0.0, 1.0, a, b)
